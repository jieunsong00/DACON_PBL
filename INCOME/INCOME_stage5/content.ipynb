{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#hiddencell\n",
        "from pbl_tools import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "fe = fm.FontEntry(fname = 'NotoSansKR-Regular.otf', name = 'NotoSansKR')\n",
        "fm.fontManager.ttflist.insert(0, fe)\n",
        "plt.rc('font', family='NotoSansKR')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 스테이지 5. 고급 데이터 전처리 및 차원 축소를 통한 모델 학습과 평가\n",
        "## 도입\n",
        "이번 스테이지에서는 고급 데이터 전처리와 차원 축소를 통한 모델 학습과 평가에 집중합니다.   \n",
        "이전 스테이지에서는 데이터의 기본 구조와 피처 상호 작용을 다뤘으며,   \n",
        "이번에는 더 깊은 수준의 데이터 전처리와 차원 축소 기술을 활용하여 모델을 개선하고자 합니다.\n",
        "\n",
        "## 학습 목표\n",
        "- 교육 수준(education)과 결혼 상태(marital.status)를 수준 카테고리로 재분류할 수 있다.\n",
        "- 파생변수를 생성하여 데이터를 보다 의미 있게 표현할 수 있다.\n",
        "- 범주형 변수를 원-핫 인코딩하여 모델 학습에 활용할 수 있다.\n",
        "- PCA (주성분 분석)를 활용하여 데이터의 차원을 축소하고 주요 특성을 추출할 수 있다.\n",
        "- PCA 주성분의 분산 설명량을 확인하고 누적 설명량을 고려하여 주성분 개수를 선택할 수 있다.\n",
        "- 데이터를 표준화하고 PCA 모델을 적용하여 데이터를 변환할 수 있다.\n",
        "- RandomForestClassifier 모델을 활용하여 데이터를 분석하고 성능을 평가할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. pandas를 이용해 csv 파일 읽어오기\n",
        "[문제 1]  \n",
        "`Pandas` 라이브러리(library)를 가져와보세요.  \n",
        "그리고 `train.csv`, `test.csv`, `sample_submission.csv` 파일을 각각 train, test, submission 변수로 읽어오세요.  \n",
        "아래 빈칸을 채워주세요.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd  \n",
        "\n",
        "train = pd.___('train.csv')  \n",
        "test = pd.___('test.csv')  \n",
        "submission = pd.___('sample_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "ensure_vals(globals(), 'train', 'test', 'submission')\n",
        "@check_safety\n",
        "def check(\n",
        "    user_train: pd.DataFrame,\n",
        "    user_test: pd.DataFrame,\n",
        "    user_submission: pd.DataFrame\n",
        "):\n",
        "    c_point1 = hasattr(user_train, 'tail')\n",
        "    c_point2 = hasattr(user_test, 'tail')\n",
        "    c_point3 = hasattr(user_submission, 'tail')\n",
        "\n",
        "    if c_point1 and c_point2 and c_point3:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(train, test, submission)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "`pd.read_csv('파일경로/파일명')` 를 활용해 보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "```python\n",
        "import pandas as pd  \n",
        "\n",
        "train = pd.read_csv('train.csv')  \n",
        "test = pd.read_csv('test.csv')  \n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.교육(education) 수준 카테고리 재분류\n",
        "\n",
        "데이터 전처리 과정에서는 종종 특성의 값을 재구분하는 작업이 필요합니다.   \n",
        "이는 카테고리형 변수의 값들을 더 큰 범주로 묶어 데이터의 복잡도를 줄이고,   \n",
        "모델 학습에 유용한 정보를 보다 명확하게 표현하는데 도움이 됩니다.\n",
        "\n",
        "우리의 데이터셋에서 'education' 특성은 사람들의 교육 수준을 나타내는 여러 가지 값으로 구성되어 있습니다.   \n",
        "그러나 이러한 각각의 값들이 모델 학습에 동일하게 중요하지 않을 수 있으며,   \n",
        "비슷한 교육 수준을 가진 카테고리들은 실질적으로 같은 의미를 가질 가능성이 있습니다.\n",
        "\n",
        "따라서 'education' 특성의 일부 값을 새로운 범주로 묶어보도록 하겠습니다.   \n",
        "이렇게 함으로써 우리는 데이터 내에서 비슷한 의미를 가진 여러 개의 범주를 단순화하여 모델 성능을 향상시키고자 합니다.\n",
        "\n",
        "[문제 2]\n",
        "\n",
        "- `replace()` 함수를 사용하여 초등학교 교육 수준('Preschool', '1st-4th', '5th-6th')과 중학교 초기('7th-8th') 및 후기('9th'), 고등학교 초기('10th', '11th') 및 후기('12th') 등을 포함하는 여러 개의 범주를 하나인 `LowEducation`으로 재구분해주세요.\n",
        "\n",
        "- 일부 고등 교육 수준(대학 일부 코스와 전문 대학 코스)은 `SomeHigherEd`로,    \n",
        "석사 학위와 전문 학위 프로그램은 모두 `Masters`로 재구분하였습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "train['education'].___(['Preschool', '10th', '11th', '12th', '1st-4th', '5th-6th', '7th-8th', '9th'], 'LowEducation', inplace=True)\n",
        "train['education'].replace(['Some-college', 'Assoc-acdm', 'Assoc-voc'], 'SomeHigherEd', inplace=True)\n",
        "train['education'].replace(['Masters', 'Prof-school'], ['Masters', 'Masters'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "ensure_vals(globals(), 'train')\n",
        "@check_safety\n",
        "def check(\n",
        "    df: pd.DataFrame,\n",
        "    col: str,\n",
        "    val1: str,\n",
        "    val2: str,\n",
        "    val3: str\n",
        "):\n",
        "    c_point0 = val1 in df[col].unique()\n",
        "    c_point1 = val2 in df[col].unique()\n",
        "    c_point2 = val3 not in df[col].unique()\n",
        "\n",
        "    if c_point0 and c_point1 and c_point2:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(train, 'education', 'LowEducation', 'SomeHigherEd', 'Prof-school')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.\n",
        "\n",
        "`inplace=True`는 데이터프레임을 직접 수정하도록 지시하는 파라미터입니다.   \n",
        "이 파라미터를 `True`로 설정하면, 해당 연산이 원래의 데이터프레임에 적용되어 수정됩니다.   \n",
        "따라서 새로운 데이터프레임을 반환하지 않고, 기존 데이터프레임이 변경됩니다.   \n",
        "이를 통해 메모리를 절약하고 코드를 간결하게 작성할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "```python\n",
        "train['education'].replace(['Preschool', '10th', '11th', '12th', '1st-4th', '5th-6th', '7th-8th', '9th'], 'LowEducation', inplace=True)\n",
        "train['education'].replace(['Some-college', 'Assoc-acdm', 'Assoc-voc'], 'SomeHigherEd', inplace=True)\n",
        "train['education'].replace(['Masters', 'Prof-school'], ['Masters', 'Masters'], inplace=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "# 3.결혼 상태 (marital.status) 수준 카테고리 재분류\n",
        "\n",
        "Never.married는 결혼을 한 번도 하지 않은 상태를 나타내며, Married.spouse.absent는 배우자가 부재한 상태를 나타냅니다.   \n",
        "이 두 상태를 통합해주겠습니다.\n",
        "\n",
        "Married.AF.spouse는 미국 공군의 배우자와 결혼한 상태를 나타내며, Married.civ.spouse는 미국 시민과 결혼한 상태를 나타냅니다.\n",
        "이 두 상태를 통합해주겠습니다.\n",
        "\n",
        "Separated는 별거 중인 상태를 나타내며, Divorced는 이혼한 상태를 나타냅니다.  \n",
        "이 두 상태를 통합해주겠습니다.\n",
        "\n",
        "[문제 3]  \n",
        "\n",
        "- `train` 데이터셋의 `marital.status` 열에서 `Never.married`와 `Married.spouse.absent`의 값을 `UnmarriedStatus`로 대체해주세요.  \n",
        "`inplace=True` 매개변수를 사용하여 원본 데이터프레임을 변경하도록 설정 해주세요.\n",
        "- marital.status 열에서 `Married.AF.spouse`와 `Married.civ.spouse`의 값을 Married로 대체해주세요.\n",
        "- marital.status 열에서 `Separated와 Divorced`의 값을 `MarriageEnded`로 대체해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train['marital.status'].___(['Never.married', 'Married.spouse.absent'], 'UnmarriedStatus', inplace=___)\n",
        "train['marital.status'].replace(['Married.AF.spouse', 'Married.civ.spouse'], '___', inplace=True)\n",
        "___[___].___([___, ___], ___, inplace=___)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "ensure_vals(globals(), 'train')\n",
        "@check_safety\n",
        "def check(\n",
        "    df: pd.DataFrame,\n",
        "    col: str,\n",
        "    val1: str,\n",
        "    val2: str,\n",
        "):\n",
        "    c_point0 = val1 in df[col].unique()\n",
        "    c_point1 = val2 not in df[col].unique()\n",
        "\n",
        "    if c_point0 and c_point1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(train, 'marital.status', 'MarriageEnded', 'Separated')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "\n",
        "replace 함수는 데이터프레임 또는 시리즈에서 특정 값을 다른 값으로 대체하는 데 사용되는 메서드입니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "```python\n",
        "train['marital.status'].replace(['Never.married', 'Married.spouse.absent'], 'UnmarriedStatus', inplace=True)\n",
        "train['marital.status'].replace(['Married.AF.spouse', 'Married.civ.spouse'], 'Married', inplace=True)\n",
        "train['marital.status'].replace(['Separated', 'Divorced'], 'MarriageEnded', inplace=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.나이(age)와 주당 근무시간(hours.per.week)을 곱한 새로운 특성 생성\n",
        "\n",
        "\n",
        "데이터 분석 과정에서는 새로운 특성을 생성함으로써 모델의 성능을 향상시키는 `특성 엔지니어링(feature engineering)`이 필요합니다.   \n",
        "이는 기존 데이터에서 추가적인 정보를 추출하거나, 데이터 간의 복잡한 관계를 캡처하는 데 도움이 됩니다.\n",
        "\n",
        "우리의 경우, `age`와 `hours.per.week`라는 두 가지 특성 사이에 중요한 상호작용(interaction)이 있음을 발견하였습니다.   \n",
        "이전 단계인 stage3에서 scatterplot을 그려본 결과, 나이와 주당 근무시간 사이에 분명한 패턴 혹은 관계가 보여,   \n",
        "이 두 변수를 조합하여 새로운 특성 `age-hours`를 생성하기로 결정하였습니다.\n",
        "\n",
        "새로운 `age-hours` 특성은 개개인의 나이와 그들이 일하는 시간 사이의 복합적인 영향력을 반영할 것입니다.   \n",
        "이렇게 함으로써 우리 모델은 나이와 근무 시간 각각만 고려하는 것보다 더 많은 정보를 얻어 성능 개선에 도움을 줄 수 있게 될 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>age</th>\n",
              "      <th>workclass</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education.num</th>\n",
              "      <th>marital.status</th>\n",
              "      <th>occupation</th>\n",
              "      <th>relationship</th>\n",
              "      <th>race</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital.gain</th>\n",
              "      <th>capital.loss</th>\n",
              "      <th>hours.per.week</th>\n",
              "      <th>native.country</th>\n",
              "      <th>target</th>\n",
              "      <th>age-hours</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TRAIN_0000</td>\n",
              "      <td>75</td>\n",
              "      <td>Self-emp-not-inc</td>\n",
              "      <td>218521</td>\n",
              "      <td>SomeHigherEd</td>\n",
              "      <td>10</td>\n",
              "      <td>Married-spouse-absent</td>\n",
              "      <td>Craft-repair</td>\n",
              "      <td>Not-in-family</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>United-States</td>\n",
              "      <td>0</td>\n",
              "      <td>2250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TRAIN_0001</td>\n",
              "      <td>23</td>\n",
              "      <td>Private</td>\n",
              "      <td>194102</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>13</td>\n",
              "      <td>Never-married</td>\n",
              "      <td>Exec-managerial</td>\n",
              "      <td>Unmarried</td>\n",
              "      <td>White</td>\n",
              "      <td>Male</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>United-States</td>\n",
              "      <td>0</td>\n",
              "      <td>920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TRAIN_0002</td>\n",
              "      <td>34</td>\n",
              "      <td>Private</td>\n",
              "      <td>238305</td>\n",
              "      <td>SomeHigherEd</td>\n",
              "      <td>10</td>\n",
              "      <td>Married-civ-spouse</td>\n",
              "      <td>Other-service</td>\n",
              "      <td>Wife</td>\n",
              "      <td>White</td>\n",
              "      <td>Female</td>\n",
              "      <td>0</td>\n",
              "      <td>1628</td>\n",
              "      <td>12</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>408</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           ID  age         workclass  fnlwgt     education  education.num  \\\n",
              "0  TRAIN_0000   75  Self-emp-not-inc  218521  SomeHigherEd             10   \n",
              "1  TRAIN_0001   23           Private  194102     Bachelors             13   \n",
              "2  TRAIN_0002   34           Private  238305  SomeHigherEd             10   \n",
              "\n",
              "          marital.status       occupation   relationship   race     sex  \\\n",
              "0  Married-spouse-absent     Craft-repair  Not-in-family  White    Male   \n",
              "1          Never-married  Exec-managerial      Unmarried  White    Male   \n",
              "2     Married-civ-spouse    Other-service           Wife  White  Female   \n",
              "\n",
              "   capital.gain  capital.loss  hours.per.week native.country  target  \\\n",
              "0             0             0              30  United-States       0   \n",
              "1             0             0              40  United-States       0   \n",
              "2             0          1628              12            NaN       0   \n",
              "\n",
              "   age-hours  \n",
              "0       2250  \n",
              "1        920  \n",
              "2        408  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train['age-hours'] = train['age']*train['hours.per.week']\n",
        "train.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#checkcode\n",
        "#empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5.데이터 분할 및 train/valid 데이터 준비\n",
        "\n",
        "[문제 4]\n",
        "- 독립변수 x에는 train 데이터셋에서 target,education 을 제외한 나머지 모든 열의 값들로 설정해 주세요.\n",
        "- 종속변수 y에는 train 데이터셋에서 target 열의 값으로 설정해 주세요.\n",
        "- 독립변수 x, 종속변수 y 데이터를 훈련 세트(train)와 검증 세트(valid)로 (7:3의 비율로) 나누어 보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = ...\n",
        "y = ...\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "___, ___, ___, ___ = ___(x, y, ___ = 0.3, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "ensure_vals(globals(), 'x_valid', 'y_valid')\n",
        "@check_safety\n",
        "def check(\n",
        "        user_answer_x : str,\n",
        "        user_answer_y : str,\n",
        "):\n",
        "    c_point0 = hasattr(user_answer_x, 'head')\n",
        "    c_point1 = hasattr(user_answer_y, 'head')\n",
        "\n",
        "    if c_point0 and c_point1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(x_valid,y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "`drop()` 함수를 사용하면 특정 칼럼을 제거할 수 있습니다.  \n",
        "`test_size` 인자로 비율을 설정할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "```python\n",
        "x = train.drop(['target','education'], axis = 1)\n",
        "y = train['target']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.3, random_state = 42)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6.결측치 처리와 불필요한 열 제거를 위한 데이터 전처리\n",
        "\n",
        "[문제 5]\n",
        "\n",
        "- 결측치 처리를 위한 `SimpleImputer` 클래스를 가져오세요.    \n",
        "- `SimpleImputer` 객체를 생성하고, 결측치를 처리할 때 해당 특성에서 가장 자주 발생하는 값으로 결측치를 대체해주세요.\n",
        "- 학습 데이터셋인 `x_train`의 'occupation'과 'workclass' 열에 대해 결측치를 `최빈값`으로 채워주세요.\n",
        "- 검증 데이터셋인 `x_valid`의 'occupation'과 'workclass' 열에 대해 결측치를 `최빈값`으로 채워주세요.\n",
        "- x_train과 x_valid 데이터셋에서 불필요한 피처인 `'native.country','ID'` 을 제거해 주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.impute import ___\n",
        "\n",
        "# SimpleImputer를 사용하여 결측치를 최빈값으로 보간\n",
        "imputer = ___(strategy='___')\n",
        "x_train[['occupation','workclass']] = ___.___(...)\n",
        "x_valid[['occupation','workclass']] = ___.___(...)\n",
        "\n",
        "# 불필요한 열 제거\n",
        "x_train = x_train.___(...)\n",
        "x_valid = x_valid.___(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "ensure_vals(globals(), 'x_train', 'imputer')\n",
        "@check_safety\n",
        "def check(\n",
        "    df: pd.DataFrame,\n",
        "    encoder: SimpleImputer,\n",
        "    not_col: str,\n",
        "    use_col1: str,\n",
        "    use_col2: str\n",
        "):\n",
        "    c_point0 = not_col not in df.columns\n",
        "    c_point1 = use_col1 in encoder.feature_names_in_\n",
        "    c_point1 = use_col2 in encoder.feature_names_in_\n",
        "\n",
        "    if c_point0 and c_point1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(x_train, imputer, 'native.country', 'workclass', 'occupation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "- 전략(strategy) 인자를 'most_frequent'로 설정하면 최빈값으로 결측치를 대체할 수 있습니다.\n",
        "- 학습 데이터셋인 `x_train`의 'occupation'과 'workclass' 열에 대해 결측치를 `최빈값`으로 채우기 위해 `imputer` 객체의 `fit_transform` 메서드를 사용합니다.   \n",
        "- 검증 데이터셋인 `x_valid`의 'occupation'과 'workclass' 열에 대해 결측치를 `최빈값`으로 채우기 위해 `imputer` 객체의 `transform` 메서드를 사용합니다.   \n",
        "- `drop()` 함수를 사용하면 특정 칼럼을 제거할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "```python\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# SimpleImputer를 사용하여 결측치를 최빈값으로 보간\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "x_train[['occupation','workclass']] = imputer.fit_transform(x_train[['occupation','workclass']])\n",
        "x_valid[['occupation','workclass']] = imputer.transform(x_valid[['occupation','workclass']])\n",
        "\n",
        "# 불필요한 열 제거\n",
        "x_train = x_train.drop(['ID','native.country'], axis = 1)\n",
        "x_valid = x_valid.drop(['ID','native.country'], axis = 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7.범주형 변수 원-핫 인코딩\n",
        "\n",
        "이전 스테이지에서는 모든 범주형 변수를 레이블 인코딩(label encoding)을 통해 수치형으로 변환하였습니다.   \n",
        "이 방법은 각 범주에 고유한 정수 값을 할당하여 `순서가 있는 특성(ordinal features)`을 처리하는 데 유용합니다.\n",
        "\n",
        "그러나 'race', 'sex', 'marital.status'와 같은 특성들은 `순서가 없는(nominal)` 범주형 변수입니다.   \n",
        "이런 경우, 각 범주를 독립적인 이진 특성으로 변환하는 `원-핫 인코딩(one-hot encoding)`이 더 적합합니다.   \n",
        "원-핫 인코딩은 모델에게 잘못된 정보(예: 하나의 범주가 다른 것보다 큰 것으로 해석될 수 있음)를 제공하는 것을 방지할 수 있습니다.\n",
        "\n",
        "sklearn의 `OneHotEncoder`를 사용하여 'race', 'sex', 'marital.status' 세 가지 특성에 대해 원-핫 인코딩을 적용하겠습니다.\n",
        "\n",
        "[문제 6]\n",
        "- scikit-learn의 preprocessing 모듈에서 `OneHotEncoder` 클래스를 불러오세요.\n",
        "- `OneHotEncoder` 객체를 생성합니다.   \n",
        "`sparse=False` 옵션은 희소 행렬(sparse matrix)이 아닌 밀집 행렬(dense matrix) 형태로 데이터를 반환하도록 설정합니다.\n",
        "- 학습 데이터셋(`x_train`)의 `'race', 'sex', 'marital.status'` 열을 선택하고,   \n",
        "`fit_transform` 메서드를 사용하여 원-핫 인코딩을 수행합니다.   \n",
        "이 작업은 학습 데이터에 대해 모델을 훈련하면서 해당 열의 범주를 기반으로 원-핫 인코딩을 학습합니다.\n",
        "- 검증 데이터셋(`x_valid`)에 대해 원-핫 인코딩을 수행합니다. 학습 데이터와 같은 열 순서 및 범주를 사용하여 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import ___\n",
        "\n",
        "# OneHotEncoder 객체 생성\n",
        "encoder = ___(sparse=False)\n",
        "\n",
        "# 학습 데이터에 대해 fit_transform 실행\n",
        "x_train_encoded = encoder.___(x_train[['race', 'sex', 'marital.status']])\n",
        "\n",
        "# 검증 데이터에 대해 transform 실행\n",
        "x_valid_encoded = encoder.transform(___[['race', 'sex', 'marital.status']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "import numpy as np\n",
        "ensure_vals(globals(), 'x_train', 'encoder', 'x_train_encoded')\n",
        "@check_safety\n",
        "def check(\n",
        "    encoder_name: str,\n",
        "    enc: OneHotEncoder,\n",
        "    col1: str,\n",
        "    col2: str,\n",
        "    onehoted: np.ndarray,\n",
        "    indexing_col: int,\n",
        "    len_col: int\n",
        "):\n",
        "    c_point0 = encoder_name in str(enc)\n",
        "    c_point1 = col1 in enc.feature_names_in_\n",
        "    c_point2 = col2 in enc.feature_names_in_\n",
        "    c_point3 = onehoted.shape[indexing_col] == 13\n",
        "\n",
        "    if (\n",
        "        c_point0 and \n",
        "        c_point1 and\n",
        "        c_point2 and \n",
        "        c_point3\n",
        "    ):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check('OneHotEncoder', encoder, 'race', 'marital.status', x_train_encoded, 1, 13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.\n",
        "원핫인코딩(One-Hot Encoding)은 범주형 데이터를 수치형 데이터로 변환하는 방법 중 하나입니다.   \n",
        "범주형 데이터는 일반적으로 문자열 레이블로 표현되는데, 머신러닝 모델은 수치 데이터를 입력으로 받아들이기 때문에 범주형 데이터를 수치형으로 변환해야 합니다.\n",
        "\n",
        "원핫인코딩의 주요 아이디어는 각 범주(카테고리)를 새로운 이진 열로 변환하는 것입니다.   \n",
        "각 열은 해당 범주에 속하면 1을, 속하지 않으면 0을 가집니다.   \n",
        "이렇게 하면 각 범주가 모델에게 독립적으로 존재하는 것처럼 다루어집니다.\n",
        "\n",
        "간단한 예를 통해 설명하겠습니다.   \n",
        "예를 들어, \"색상\"이라는 범주형 특성이 있고, 이 특성에는 \"빨강\", \"녹색\", \"파랑\"과 같은 카테고리가 있다고 가정합니다.   \n",
        "이를 원핫인코딩으로 변환하면 다음과 같이 됩니다:\n",
        "\n",
        "\"빨강\" 카테고리는 [1, 0, 0]\n",
        "\"녹색\" 카테고리는 [0, 1, 0]\n",
        "\"파랑\" 카테고리는 [0, 0, 1]\n",
        "따라서 원핫인코딩된 결과는 세 개의 새로운 열(빨강, 녹색, 파랑)로 나타나며, 각 행은 하나의 카테고리에 대응합니다.\n",
        "\n",
        "원핫인코딩은 머신러닝 모델에 범주형 데이터를 제공하기 위한 일반적인 방법 중 하나이며, 모델이 범주 정보를 잘 이해할 수 있도록 도와줍니다.   \n",
        "그러나 원핫인코딩을 적용하면 범주의 개수가 많아질수록 데이터 차원이 증가하므로, 차원의 저주(curse of dimensionality) 문제에 유의해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "```python\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# OneHotEncoder 객체 생성\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "\n",
        "# 학습 데이터에 대해 fit_transform 실행\n",
        "x_train_encoded = encoder.fit_transform(x_train[['race', 'sex', 'marital.status']])\n",
        "\n",
        "# 검증 데이터에 대해 transform 실행\n",
        "x_valid_encoded = encoder.transform(x_valid[['race', 'sex', 'marital.status']])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8.원-핫 인코딩된 데이터를 DataFrame으로 변환\n",
        "\n",
        "scikit-learn의 OneHotEncoder가 반환하는 것은 numpy 배열 형태입니다.     \n",
        "이는 기계 학습 모델에 바로 입력할 수 있는 형태이지만, 우리가 데이터를 직접 다루거나 분석할 때는 pandas DataFrame 형태가 더 편리합니다.     \n",
        "DataFrame은 레이블링된 열(column)을 가지므로 어떤 열이 어떤 원래의 범주에 해당하는지 쉽게 파악할 수 있습니다.\n",
        "\n",
        "따라서 원-핫 인코딩된 numpy 배열을 pandas DataFrame으로 변환해보도록 하겠습니다. \n",
        "\n",
        "[문제 7]  \n",
        "`pd.DataFrame()` 메소드를 사용하여 원-핫 인코딩된 훈련 데이터를 새로운 데이터프레임으로 변환합니다.   \n",
        "columns 인자에는 `get_feature_names_out()` 메소드를 사용하여 각각의 컬럼 이름을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train_ohe = pd.___(x_train_encoded, columns=encoder.___(['race', 'sex', 'marital.status']))\n",
        "x_valid_ohe = pd.DataFrame(x_valid_encoded, columns=encoder.get_feature_names_out(['race', 'sex', 'marital.status']))\n",
        "\n",
        "x_train_ohe.head(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "ensure_vals(globals(), 'x_train_ohe')\n",
        "@check_safety\n",
        "def check(\n",
        "    df: pd.DataFrame,\n",
        "    shape_df: tuple\n",
        "):\n",
        "    c_point0 = df.shape == shape_df\n",
        "\n",
        "    if c_point0:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(x_train_ohe, (3108, 13))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.\n",
        "\n",
        "원-핫 인코딩된 훈련 데이터(`x_train_encoded`)를 새로운 데이터프레임으로 변환합니다.  \n",
        "원-핫 인코딩된 특성의 컬럼 이름을 생성합니다.   \n",
        "\n",
        "이 데이터프레임은 원-핫 인코딩된 특성들을 가집니다.  \n",
        "원-핫 인코딩된 검증 데이터도 마찬가지로 새로운 데이터프레임으로 변환합니다.\n",
        "\n",
        "이러한 과정을 통해 'race', 'sex', 'marital.status' 특성을 원-핫 인코딩하여 새로운 데이터프레임으로 생성하게 됩니다.   \n",
        "이렇게 하면 기존의 범주형 특성이 머신러닝 모델에 적용할 수 있도록 숫자로 변환됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hiny.\n",
        "empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "```python\n",
        "x_train_ohe = pd.DataFrame(x_train_encoded, columns=encoder.get_feature_names_out(['race', 'sex', 'marital.status']))\n",
        "x_valid_ohe = pd.DataFrame(x_valid_encoded, columns=encoder.get_feature_names_out(['race', 'sex', 'marital.status']))\n",
        "\n",
        "x_train_ohe.head(4)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9.데이터프레임에 원-핫 인코딩된 특성 추가\n",
        "\n",
        "데이터 처리 과정에서는 종종 여러 데이터프레임을 합치는 작업이 필요합니다.   \n",
        "이번에는 원-핫 인코딩된 데이터프레임(x_train_ohe, x_valid_ohe)과 원래의 데이터프레임(x_train, x_valid)을 합칠 것입니다.\n",
        "\n",
        "pandas의 concat 함수를 사용하면 간단하게 두 데이터프레임을 합칠 수 있습니다.   \n",
        "하지만 주의할 점은, concat 함수는 기본적으로 각 데이터프레임의 인덱스를 따라 동작한다는 것입니다.   \n",
        "즉, 서로 다른 인덱스를 가진 두 데이터프레임을 합치게 되면 예상하지 못한 결과가 나올 수 있습니다.\n",
        "\n",
        "따라서 우리는 먼저 reset_index 메소드를 사용하여 x_train과 x_valid의 인덱스를 초기화하겠습니다.    \n",
        "인덱스가 초기화된 후에야 비로소 원-핫 인코딩된 피처들과 원래 피처들이 올바르게 연결시켜주겠습니다.\n",
        "\n",
        "\n",
        "\n",
        "[문제 8]\n",
        "- `reset_index(drop=True)`를 사용하여 기존의 인덱스를 삭제하고 새로운 연속적인 정수 인덱스로 설정해주세요.\n",
        "- x_train_ohe와 x_valid_ohe를 `concat()` 함수를 사용하여   \n",
        "각각의 기존 데이터프레임 x_train과 x_train_ohe을 열 방향(axis=1)으로 합쳐주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = x_train.___(drop=___)\n",
        "x_valid = x_valid.reset_index(drop=True)\n",
        "\n",
        "x_train = pd.___([x_train,x_train_ohe], axis=1)\n",
        "x_valid = pd.concat([x_valid,x_valid_ohe], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "ensure_vals(globals(), 'x_train', 'x_valid')\n",
        "@check_safety\n",
        "def check(\n",
        "    train_df: pd.DataFrame,\n",
        "    val_df: pd.DataFrame,\n",
        "    train_shape: tuple,\n",
        "    val_shape: tuple\n",
        "):\n",
        "    c_point0 = train_df.shape == train_shape\n",
        "    c_point1 = val_df.shape == val_shape\n",
        "\n",
        "    if c_point0 and c_point1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(x_train, x_valid, (3108,26), (1332,26))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.\n",
        "x_train과 x_valid 데이터프레임의 인덱스를 재설정합니다.   \n",
        "reset_index(drop=True)를 사용하여 기존의 인덱스를 삭제하고 새로운 연속적인 정수 인덱스로 설정합니다.   \n",
        "이렇게 하면 인덱스가 재정렬되고 데이터프레임이 정리됩니다.\n",
        "\n",
        "원-핫 인코딩된 데이터프레임 x_train_ohe와 x_valid_ohe를 pd.concat 함수를 사용하여   \n",
        "각각의 기존 데이터프레임 x_train과 x_valid에 열 방향(axis=1)으로 합칩니다.   \n",
        "이렇게 하면 기존의 특성들과 원-핫 인코딩된 새로운 특성들이 함께 있는 새로운 데이터프레임이 생성됩니다.\n",
        "\n",
        "이제 새로운 데이터프레임을 사용하여 머신러닝 모델을 학습하고 예측할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "```python\n",
        "x_train = x_train.reset_index(drop=True)\n",
        "x_valid = x_valid.reset_index(drop=True)\n",
        "\n",
        "x_train = pd.concat([x_train,x_train_ohe], axis=1)\n",
        "x_valid = pd.concat([x_valid,x_valid_ohe], axis=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10.LabelEncoder를 사용한 범주형 데이터 인코딩\n",
        "\n",
        "이전에 우리는 순서가 없는 범주형 피처들에 대해 원-핫 인코딩을 적용하였습니다.   \n",
        "그러나 아직 처리하지 않은 다른 범주형 피처들도 있습니다.   \n",
        "이들은 순서가 있는 데이터일 가능성이 있으므로, 레이블 인코딩(label encoding)을 적용하려 합니다.   \n",
        "레이블 인코딩은 각 고유한 범주값에 대해 고유한 정수값을 할당하는 방식으로, 순서 정보를 유지할 수 있게 합니다.\n",
        "\n",
        "[문제 9]   \n",
        "- `LabelEncoder` 객체를 생성하여 le 변수에 할당해 주세요.\n",
        "- `x_train` 데이터프레임의 현재 열에 LabelEncoder을 적용해 주세요.\n",
        "- 만약 검증 데이터에서 새롭게 나타나는 범주값(label)이라면, 이를 인코더의 클래스 목록(le.classes_)에 추가해 주세요.\n",
        "- `x_valid` 데이터프레임의 현재 열에 LabelEncoder을 적용해 주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "for col in x_train.columns:\n",
        "    if x_train[col].dtype == 'object':\n",
        "        \n",
        "        le = ___()\n",
        "        x_train[col] = ___.___(___[___])\n",
        "\n",
        "        for label in np.unique(x_valid[col]):\n",
        "            if label not in le.classes_:\n",
        "                le.classes_ = np.___(le.classes_, ___)\n",
        "        x_valid[col] = ___.___(___[___])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "ensure_vals(globals(), 'x_train', 'x_valid')\n",
        "@check_safety\n",
        "def check(\n",
        "    train_df: pd.DataFrame,\n",
        "    val_df: pd.DataFrame,\n",
        "    obj: str,\n",
        "    zero: 0\n",
        "):\n",
        "    \n",
        "    len_obj_train = len(train_df.select_dtypes(include=obj).columns)\n",
        "    len_obj_test = len(val_df.select_dtypes(include=obj).columns)\n",
        "    \n",
        "    c_point0 = len_obj_train == zero\n",
        "    c_point1 = len_obj_test == zero\n",
        "\n",
        "    if c_point0:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(x_train, x_valid, 'object', 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "- `sklearn.preprocessing` 모듈에서 `LabelEncoder`라는 클래스를 불러올 수 있습니다.\n",
        "- `fit_transform` 메서드를 사용하면 LabelEncoder를 적용할 수 있습니다.\n",
        "- `append()` 메서드를 사용하면 클래스 목록에 추가할 수 있습니다.\n",
        "- `transform` 메서드를 사용하여 x_valid 데이터에 Label Encoding을 적용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "for col in x_train.columns:\n",
        "    if x_train[col].dtype == 'object':\n",
        "        \n",
        "        le = LabelEncoder()\n",
        "        x_train[col] = le.fit_transform(x_train[col])\n",
        "\n",
        "        for label in np.unique(x_valid[col]):\n",
        "            if label not in le.classes_:\n",
        "                le.classes_ = np.append(le.classes_, label)\n",
        "        x_valid[col] = le.transform(x_valid[col])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 11.StandardScaler를 이용한 데이터 표준화\n",
        "\n",
        "데이터 전처리는 머신러닝 모델의 성능에 중요한 역할을 합니다.   \n",
        "이 과정에서 데이터의 특성들을 적절하게 변환하여 모델이 데이터를 더 잘 이해할 수 있도록 돕습니다.   \n",
        "그 중 하나가 바로 특성 스케일링입니다.\n",
        "\n",
        "특성들 간에 값의 범위가 다르면, 그 값이 큰 특성이 결과에 더 큰 영향을 주게 됩니다.   \n",
        "예를 들어, '나이'는 보통 0-100 사이의 값을 가지지만, '수입'은 수 천에서 수 만까지 다양하게 분포할 수 있습니다.   \n",
        "이런 경우 '수입' 특성이 결과에 과도하게 영향을 미치게 되므로, 모든 특성이 동등하게 반영되도록 스케일링 작업을 거치는 것입니다.\n",
        "\n",
        "[문제 10]\n",
        "- scikit-learn 라이브러리에서 제공하는 `StandardScaler` 클래스를 사용하여 scaler 객체를 생성해주세요.\n",
        "\n",
        "- 학습 데이터셋(`x_train`)에 대해서는 `fit_transform` 메서드를 사용하여 평균과 분산을 계산(fit)한 후   \n",
        "표준화(transform)를 진행합니다.\n",
        "\n",
        "- 검증 데이터셋(`x_valid`)에 대해서는 `transform` 메서드를 사용하여   \n",
        "학습 데이터셋에서 이미 계산된 평균과 분산 값을 사용하여 표준화(transform)를 진행합니다.   \n",
        "이렇게 해야 학습과 검증 단계에서 일관된 전처리가 적용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "features = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss','hours.per.week', 'age-hours']\n",
        "\n",
        "scaler = ___()\n",
        "x_train[features] = scaler.___(x_train[___])\n",
        "x_valid[features] = ___.___(x_valid[features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "ensure_vals(globals(), 'x_train', 'x_valid', 'scaler')\n",
        "@check_safety\n",
        "def check(\n",
        "    train_df: pd.DataFrame,\n",
        "    val_df: pd.DataFrame,\n",
        "    scaler_class: StandardScaler,\n",
        "    num_features: int\n",
        "):\n",
        "    \n",
        "    c_point0 = len(scaler_class.feature_names_in_) == num_features\n",
        "\n",
        "    if c_point0:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(x_train, x_valid, scaler, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.\n",
        "해당 코드는 데이터의 특성(features)을 표준화하는 과정을 나타냅니다.   \n",
        "특성들의 스케일이 서로 다르면, 일부 특성들이 모델 학습에 과도하게 영향을 미칠 수 있기 때문에 이런 스케일링 작업은 매우 중요합니다.\n",
        "\n",
        "위 코드에서는 scikit-learn의 `StandardScaler`를 사용하여 표준화를 진행합니다.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "features = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss','hours.per.week', 'age-hours']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train[features] = scaler.fit_transform(x_train[features])\n",
        "x_valid[features] = scaler.transform(x_valid[features])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 12.데이터 차원 축소를 위한 PCA 모델 학습\n",
        "\n",
        "이번에는 주성분 분석(Principal Component Analysis, PCA)을 사용하여 데이터의 차원을 축소하는 작업을 수행해봅시다.   \n",
        "PCA는 다차원 데이터를 저차원 공간으로 변환하여 데이터의 변동성을 최대한 보존하는 방식으로 동작합니다.\n",
        "\n",
        "[문제 11]  \n",
        "- `Scikit-learn`의 `decomposition` 모듈에서 `PCA` 클래스를 불러와 보세요.\n",
        "- `PCA` 객체를 생성해보세요.\n",
        "- 주어진 훈련 데이터인 `x_train에` 대해 `PCA` 모델을 적용(`fit`)시켜보세요. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.___ import PCA\n",
        "\n",
        "pca = PCA()\n",
        "pca.___(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "ensure_vals(globals(), 'x_train', 'pca')\n",
        "@check_safety\n",
        "def check(\n",
        "    train_df: pd.DataFrame,\n",
        "    pca_class\n",
        "):\n",
        "    \n",
        "    c_point0 = len(train_df.columns) == len(pca_class.get_feature_names_out())\n",
        "    \n",
        "    if c_point0:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(x_train, pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.\n",
        "PCA(Principal Component Analysis)는 다차원 데이터의 차원을 축소하고, 데이터의 주요한 패턴과 구조를 추출하는 방법입니다.\n",
        "\n",
        "PCA는 고차원 데이터셋에서 중요한 정보를 유지하면서 차원을 줄이기 위해 사용됩니다.   \n",
        "이를 위해 PCA는 기존 변수들의 선형 조합으로 이루어진 새로운 변수들인 주성분(Principal Component)을 생성합니다.   \n",
        "주성분은 원본 변수들 간의 상관관계와 분산을 고려하여 계산됩니다.\n",
        "\n",
        "주성분은 원본 데이터셋에서 가장 많은 분산을 설명하는 첫 번째 주성분부터 순서대로 나열됩니다.   \n",
        "첫 번째 주성분은 가장 많은 분산을 설명하며, 두 번째, 세 번째 순서로 오는 주성분들이 남은 분산을 설명합니다.   \n",
        "이렇게 생성된 주성분들은 서로 직교하며, 상관관계가 없습니다.\n",
        "\n",
        "PCA를 사용하여 차원 축소를 수행하면, 원래 변수보다 적은 수의 주성분만 선택하여 데이터를 표현할 수 있습니다. \n",
        "\n",
        "또한 PCA는 데이터셋 내의 패턴과 구조를 파악하는 데도 사용됩니다.   \n",
        "각 주성분에 대응하는 계수 벡터(coefficient vector)를 통해 어떤 변수가 해당 주성분에 큰 영향력을 미치는지 확인할 수 있습니다.\n",
        "\n",
        "요약하자면, PCA는 다차원 데이터셋에서 중요한 정보를 추출하고 차원 축소하기 위한 방법으로 활용되며,   \n",
        "원본 변수들 간의 상관관계와 분산을 고려하여 새로운 변수인 주성분을 생성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(x_train)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 13.PCA 주성분 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-4.79055732e-04,  2.88123473e-03,  7.14129075e-03,  2.93943613e-02,\n",
              "       -1.19098206e-02,  9.96784763e-01, -5.38655166e-02,  5.70556703e-05,\n",
              "        1.49095433e-02,  1.10344715e-02,  2.16331617e-03,  3.14708243e-02,\n",
              "        2.42733518e-02,  6.85156857e-04,  7.17858710e-05, -1.44281378e-03,\n",
              "       -1.27413157e-04,  8.13284206e-04, -1.49095433e-02,  1.49095433e-02,\n",
              "       -1.29029930e-03, -1.05222737e-04,  7.49006552e-03, -9.83909787e-06,\n",
              "       -3.66831033e-03, -2.41639405e-03])"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pca.components_[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#checkcode\n",
        "#empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.\n",
        "\n",
        "`pca.components_`는 PCA를 통해 추출된 주성분(Principal Component)들을 나타내는 속성입니다.   \n",
        "이 속성은 주성분들의 계수 벡터(coefficient vector)로 구성된 배열을 반환합니다.\n",
        "\n",
        "주성분의 계수 벡터는 원본 변수들과의 선형 조합으로 이루어져 있습니다.   \n",
        "각 주성분에 대한 계수 벡터의 크기와 부호는 해당 변수가 해당 주성분에 어떤 영향을 미치는지를 나타냅니다.\n",
        "\n",
        "해당 코드를 살펴봅시다.  \n",
        "`pca.components_[0]`은 첫 번째 주성분에 대한 계수 벡터입니다.   \n",
        "이 벡터의 요소들은 원본 변수들과 첫 번째 주성분 간의 상관관계 및 영향력을 나타냅니다.   \n",
        "요소 값이 양수인 경우 해당 변수가 첫 번째 주성분과 양의 상관관계를 가지며, 음수인 경우 음의 상관관계를 가진다고 해석할 수 있습니다.   \n",
        "또한, 값이 절대값으로 클수록 해당 변수가 해당 주성분에 큰 영향력을 미친다고 볼 수 있습니다.\n",
        "\n",
        "따라서 `pca.components_` 배열은 각각의 주성분이 원본 변수들과 어떤 관련이 있는지,   \n",
        "그리고 데이터셋에서 어떤 패턴이나 구조를 설명하는지에 대한 정보를 제공합니다.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 14.PCA 주성분의 분산 설명량 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "explained_variance_ratio: \n",
            " [4.94515588e-01 1.30375995e-01 7.19915105e-02 4.47434711e-02\n",
            " 3.84268750e-02 3.59899981e-02 3.44973959e-02 3.34197805e-02\n",
            " 3.08658290e-02 3.02436508e-02 2.74980741e-02 1.31476642e-02\n",
            " 7.37395656e-03 2.32056637e-03 1.89438084e-03 1.12435680e-03\n",
            " 5.87351407e-04 5.22480355e-04 4.16154596e-04 4.49208450e-05\n",
            " 3.48816875e-32 1.90644454e-32 3.10229048e-33 3.10229048e-33\n",
            " 3.10229048e-33 3.01745573e-33]\n",
            "\n",
            "explained_variance_ratio[0]:  0.4945155881447502\n",
            "\n",
            "explained_variance_ratio[1]:  0.13037599498893046\n"
          ]
        }
      ],
      "source": [
        "# 분산의 설명량 확인\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "print('explained_variance_ratio: \\n',explained_variance_ratio)\n",
        "print('\\nexplained_variance_ratio[0]: ',explained_variance_ratio[0])\n",
        "print('\\nexplained_variance_ratio[1]: ',explained_variance_ratio[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#checkcode\n",
        "#empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.\n",
        "\n",
        "`explained_variance_ratio_`는 PCA를 통해 추출된 주성분들이 설명하는 분산의 비율을 나타내는 속성입니다.   \n",
        "이 값은 각 주성분이 전체 분산 중 얼마나 많은 비율을 설명하는지를 나타냅니다.\n",
        "\n",
        "예를 들어, `explained_variance_ratio_[0]`은 첫 번째 주성분이 전체 분산 중 어느 정도 설명하는지를 나타냅니다.   \n",
        "이 값이 0.3이라면 첫 번째 주성분만 사용하여도 전체 분산의 약 30% 정도를 설명할 수 있다는 의미입니다.   \n",
        "마찬가지로, `explained_variance_ratio_[1]`은 두 번째 주성분의 설명력을 나타내며,   \n",
        "이 값이 0.2라면 두 번째 주성분만 사용하여 전체 분산의 약 20% 정도를 설명할 수 있다는 것을 의미합니다.\n",
        "\n",
        "주성분들의 explained variance ratio 값을 합하면 1이 됩니다.   \n",
        "따라서 모든 주성분들의 explained variance ratio 값을 합하면 데이터셋에서 차원 축소된 후에도 원래 데이터셋에서 보존되는 정보(변동)의 총량인 100%가 됩니다.\n",
        "\n",
        "`explained_variance_ratio_` 속성은 PCA 결과에서 각각의 주성분들이 데이터셋 내에서 얼마나 중요한 정보를 포함하고 있는지에 대한 상대적인 지표로 활용됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 15.누적 설명량과 주성분 개수 선택\n",
        "\n",
        "주성분 분석(PCA)은 데이터의 차원을 축소하기 위한 기술 중 하나입니다.   \n",
        "보통, 축소할 차원을 임의로 정하기 보다는 충분한 분산이 될 때까지 더해야할 차원 수를 선택하는 쪽이 더 일반적입니다.   \n",
        "이 방법을 사용하면 데이터의 정보를 최대한 보존하면서도 차원을 줄일 수 있습니다.   \n",
        "\n",
        "`cumulative_explained_variance`를 계산하여 누적 설명량이 95% 이상이 되는 주성분 개수를 선택해보겠습니다.\n",
        "\n",
        "[문제 12]  \n",
        "- `cumsum` 함수를 사용하여 `explained_variance_ratio_` 값을 누적해서 더해주세요.\n",
        "- `argmax` 함수를 사용하여 해당 임계값을 넘는 최초의 인덱스를 찾아보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 누적 설명량 계산\n",
        "cumulative_explained_variance = np.___(explained_variance_ratio)\n",
        "\n",
        "# 적절한 주성분 개수 선택 (예: 95% 이상의 누적 설명량을 가지는 개수)\n",
        "n_components = np.___(cumulative_explained_variance >= 0.95) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "import numpy as np\n",
        "ensure_vals(globals(), 'x_train', 'cumulative_explained_variance')\n",
        "@check_safety\n",
        "def check(\n",
        "    train_df: pd.DataFrame,\n",
        "    variance: np.ndarray\n",
        "):\n",
        "    \n",
        "    c_point0 = len(variance) == len(train_df.columns)\n",
        "    \n",
        "    if c_point0:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(x_train, cumulative_explained_variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.\n",
        "PCA 모델이 데이터에 적용된 후, 주성분의 설명된 분산의 비율을 확인합니다.   \n",
        "`explained_variance_ratio_` 속성을 통해 각 주성분이 설명하는 분산의 비율을 얻을 수 있습니다.  \n",
        "\n",
        "그런 다음 각 주성분이 설명하는 분산의 비율인 `explained_variance_ratio_` 값을 누적해서 더합니다.  \n",
        "이것은 주성분을 추가할 때마다 설명되는 분산의 누적 비율을 나타냅니다.       \n",
        "누적 설명량이 얼마나 되는지 확인하면 데이터의 정보를 어느 정도 보존할 수 있는지 알 수 있습니다.\n",
        "\n",
        "마지막으로, 사용자가 설정한 임계값(예: 95% 이상)을 충족하는 주성분 개수를 선택합니다.     \n",
        "이를 위해 `cumulative_explained_variance`에서 해당 임계값을 넘는 최초의 인덱스를 찾습니다.    \n",
        "(누적 설명 분산이 처음으로 0.95(즉, 전체 분산의 95%) 이상인 위치(index)를 반환합니다.)   \n",
        "여기에 1을 더해주어야 실제 필요한 최소 주성분 개수(`n_components`)가 됩니다 (Python 인덱스는 0부터 시작하기 때문).  \n",
        "\n",
        "즉, 해당 코드는 **충분한 정보(여기서는 전체 변동량의 95%)가 유지될 때까지 필요한 최소한의 차원 수를 선택하는 방법** 입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "```python\n",
        "# 누적 설명량 계산\n",
        "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# 적절한 주성분 개수 선택 (예: 95% 이상의 누적 설명량을 가지는 개수)\n",
        "n_components = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 16.PCA 모델 재설정 및 데이터 변환\n",
        "\n",
        "위에서 선택된 주성분 개수로 PCA 모델을 다시 초기화하고, 이전과 같은 데이터에 적용하여 차원을 축소합니다.   \n",
        "이렇게 함으로써 데이터의 차원을 축소하면서 원본 데이터의 정보를 가능한 한 보존하게 됩니다.\n",
        "\n",
        "[문제 14]  \n",
        "- 주성분 분석(PCA) 모델을 초기화해주세요.   \n",
        "이때, `n_components` 매개변수에는 이전 단계에서 선택한 주성분 개수인 `n_components`를 전달합니다.   \n",
        "\n",
        "- 초기화된 PCA 모델을 기존의 훈련 데이터인 `x_train`에 다시 적용(`fit_transform`)해주세요.   \n",
        "검증 데이터인 `x_valid`에 동일한 PCA 모델을 적용(`transform`)합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 선택된 주성분 개수로 PCA 모델 다시 초기화\n",
        "pca = ___(n_components=___)\n",
        "\n",
        "# 데이터에 선택된 주성분 개수로 PCA 모델 적용\n",
        "x_train = pca.___(___)\n",
        "x_valid = pca.___(___)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "import numpy as np\n",
        "ensure_vals(globals(), 'pca')\n",
        "@check_safety\n",
        "def check(\n",
        "    train_df: pd.DataFrame,\n",
        "    pca_class: PCA\n",
        "):\n",
        "    \n",
        "    c_point0 = len(pca_class.get_feature_names_out()) == pca_class.n_components\n",
        "    \n",
        "    if c_point0:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(x_train, pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.\n",
        "\n",
        "`PCA(n_components=n_components)`는 주성분 분석(PCA) 모델을 초기화하는 부분입니다.   \n",
        "이때, `n_components` 매개변수에는 이전 단계에서 선택한 주성분 개수인 `n_components`를 전달합니다.    \n",
        " 이것은 주성분 분석 모델을 새로운 주성분 개수로 초기화하는 것을 의미합니다.\n",
        "\n",
        "그 다음, 초기화된 PCA 모델을 기존의 훈련 데이터인 `x_train`에 다시 적용합니다.   \n",
        "이를 통해 데이터의 차원을 실제로 축소합니다.   \n",
        "차원 축소는 새로운 주성분 개수로 데이터를 투영하는 것을 의미하며,   \n",
        "훈련 데이터인 `x_train`과 검증 데이터인 `x_valid`에 동일한 PCA 모델을 적용합니다.\n",
        "\n",
        "이제 데이터는 이전보다 적은 주성분을 가진 형태로 변환되며, 주성분 분석을 통해 데이터의 차원이 축소됩니다.   \n",
        "이렇게 차원을 축소하면 데이터의 크기를 줄이고, 불필요한 정보를 제거하면서 모델을 더 간결하게 만들 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "```python\n",
        "# 선택된 주성분 개수로 PCA 모델 다시 초기화\n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "# 데이터에 선택된 주성분 개수로 PCA 모델 적용\n",
        "x_train = pca.fit_transform(x_train)\n",
        "x_valid = pca.transform(x_valid)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 17.RandomForestClassifier 모델을 활용한 데이터 분석 및 성능 평가\n",
        "\n",
        "`x_train`과 `x_valid` 데이터셋은 이미 PCA를 적용하여 주성분으로 변환된 데이터셋입니다. \n",
        "\n",
        "PCA 후에 남아 있는 각각의 주성분들은 원래 특성 공간에서 어떠한 방향(즉, 벡터)를 나타내며,   \n",
        "이러한 방향들은 원래 데이터가 가장 많이 분산되어 있는 방향입니다.   \n",
        "따라서 PCA로 변환된 `x_train`과 `x_valid`는 원래의 다차원 공간에서 가장 정보가 많이 담긴 방향으로 투영된 상태라고 볼 수 있습니다.\n",
        "\n",
        "그런 다음 RandomForestClassifier를 사용하여 모델을 학습시킨 후  \n",
        "마지막으로 F1 score를 계산하여 모델 성능을 평가하겠습니다.\n",
        "\n",
        "\n",
        "[문제 15]\n",
        "\n",
        "- RandomForestClassifier 모델을 생성해 주세요. random_state 는 42로 설정하여 재현성을 보장해주세요.\n",
        "- 학습 데이터를 사용하여 학습시켜주세요.\n",
        "- 학습된 모델을 사용하여 검증 데이터에 대한 예측을 수행합니다.\n",
        "- f1_score 함수를 사용하여 실제 레이블인 검증 데이터의 목표값과 모델의 예측 결과를 비교하여 Macro F1 스코어를 계산해보세요. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Macro F1 스코어: 0.7929340560259506\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "rf_classifier = ___(random_state=___)\n",
        "___.fit(___, ___)\n",
        "\n",
        "y_pred = ___.___(x_valid)\n",
        "\n",
        "macro_f1 = f1_score(y_valid, y_pred, average='___')\n",
        "print(\"Macro F1 스코어:\", macro_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 181,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "import numpy as np\n",
        "ensure_vals(globals(), 'x_train', 'y_train', 'rf_classifier', 'y_pred', 'macro_f1')\n",
        "@check_safety\n",
        "def check(\n",
        "    x_train_df: pd.DataFrame,\n",
        "    y_train_df: pd.DataFrame,\n",
        "    x_valid_df: pd.DataFrame,\n",
        "    val_pred: np.ndarray,\n",
        "    model: RandomForestClassifier,\n",
        "    model_name: str\n",
        "):\n",
        "    \n",
        "    c_point0 = model_name in str(model)\n",
        "    c_point1 = len(model.classes_) == y_train_df.nunique()\n",
        "    c_point2 = len(val_pred) == len(x_valid_df)\n",
        "    \n",
        "    if c_point0:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(x_train, y_train, x_valid, y_pred, rf_classifier, 'RandomForestClassifier')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.\n",
        "\n",
        "---\n",
        "\n",
        "#### 결과 해석\n",
        "\n",
        "결과 스코어를 확인해보면 이전 스테이지보다 하락한 것을 확인할 수 있습니다.\n",
        "PCA와 표준화를 적용한 결과가 적용하지 않은 결과보다 성능이 떨어진 것은, PCA가 데이터의 차원을 축소하면서 일부 정보를 손실시키고, 이로 인해 모델이 학습할 수 있는 유용한 정보가 줄어들었을 가능성이 있습니다. 또한, 모든 데이터셋에 대해 PCA와 표준화가 성능 향상을 가져오는 것은 아닙니다. 주어진 문제나 데이터셋의 특성에 따라 이러한 전처리 방법의 효과는 달라질 수 있습니다.\n",
        "\n",
        "또한, 검증 성능이 떨어졌다고 해서 반드시 test 데이터의 성능도 떨어지는 것은 아닙니다. 검증 점수와 실제 예측 성능 간에 차이가 발생하는 경우도 있습니다. 검증 데이터셋과 테스트 데이터셋 사이의 분포나 특성 차이 때문일 수 있으며, 모델 학습 시 사용된 알고리즘이 overfitting 혹은 underfitting 등으로 인해 일반화 성능에 영향을 줄 수도 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "- `RandomForestClassifier` 모델을 생성하여 rf_classifier 변수에 할당해 주세요.\n",
        "- 학습 데이터는 x_train, y_train 입니다.\n",
        "- 검증 데이터는 x_valid 입니다.\n",
        "- 검증 데이터의 목표값은 y_valid, 모델의 예측 결과는 y_pred 입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "rf_classifier.fit(x_train, y_train)\n",
        "\n",
        "y_pred = rf_classifier.predict(x_valid)\n",
        "\n",
        "macro_f1 = f1_score(y_valid, y_pred, average='macro')\n",
        "print(\"Macro F1 스코어:\", macro_f1)\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
