{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#hiddencell\n",
        "from pbl_tools import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "fe = fm.FontEntry(fname = 'NotoSansKR-Regular.otf', name = 'NotoSansKR')\n",
        "fm.fontManager.ttflist.insert(0, fe)\n",
        "plt.rc('font', family='NotoSansKR')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 스테이지 6. test 데이터 전처리 및 모델 학습과 평가\n",
        "## 도입\n",
        "\n",
        "이번 스테이지에서는 'test 데이터 전처리 및 모델 학습'에 중점을 두고 진행됩니다.   \n",
        "이전 스테이지에서는 데이터의 기본적인 구조와 통계량을 확인하였으며,   \n",
        "이제는 테스트 데이터를 다루며 데이터 전처리와 모델 학습을 통해 예측 모델을 고도화하고자 합니다.\n",
        "\n",
        "## 학습 목표\n",
        "- 테스트 데이터에 대한 전처리를 수행하고 필요한 새로운 특성을 생성할 수 있다.\n",
        "- 독립 변수와 종속 변수를 명확히 분리할 수 있다.\n",
        "- PCA (주성분 분석) 모델을 활용하여 데이터의 차원을 축소하고 주요 특성을 추출할 수 있다.\n",
        "- GradientBoostingClassifier 모델을 활용하여 테스트 데이터를 분석하고 모델 학습을 진행할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. 데이터 전처리 및 새로운 특성 생성\n",
        "[문제 1]  \n",
        "- education 피처의 `Some-college, Assoc-acdm, Assoc-voc` 값들을 `SomeHigherEd`로 재구분해주세요. 이를 원래의 데이터프레임에 적용될 수 있도록 해주세요.\n",
        "- marital.status 피처의 `'Separated', 'Divorced'` 값들을 `MarriageEnded` 로 재구분해주세요. 이를 원래의 데이터프레임에 적용될 수 있도록 해주세요.\n",
        "- `age` 피처와 `hours.per.week` 피처를 곱한 `age-hours` 파생변수를 생성해 주세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd  \n",
        "\n",
        "train = pd.read_csv('train.csv')  \n",
        "test = pd.read_csv('test.csv')  \n",
        "\n",
        "# 교육(education) 수준 카테고리 재분류\n",
        "train['education'].replace(['Preschool', '10th', '11th', '12th', '1st-4th', '5th-6th', '7th-8th', '9th'], 'LowEducation', inplace=True)\n",
        "train['education'].replace([___, ___, ___], ___, inplace=___)\n",
        "train['education'].replace(['Masters', 'Prof-school'], ['Masters', 'Masters'], inplace=True)\n",
        "\n",
        "#결혼 상태 (marital.status) 수준 카테고리 재분류\n",
        "train['marital.status'].replace(['Never.married', 'Married.spouse.absent'], 'UnmarriedStatus', inplace=True)\n",
        "train['marital.status'].replace(['Married.AF.spouse', 'Married.civ.spouse'], 'Married', inplace=True)\n",
        "train['marital.status'].replace(...)\n",
        "\n",
        "# 나이(age)와 주당 근무시간(hours.per.week)을 곱한 새로운 특성 생성\n",
        "train['age-hours'] = train[___] * train[___]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "ensure_vals(globals(), 'train')\n",
        "@check_safety\n",
        "def check(\n",
        "    df: pd.DataFrame,\n",
        "    col: str,\n",
        "    val1: str,\n",
        "    val2: str,\n",
        "    val3: str,\n",
        "    new_col: str\n",
        "):\n",
        "    c_point0 = hasattr(df, 'head')\n",
        "    c_point1 = val1 in df[col].unique()\n",
        "    c_point2 = val2 in df[col].unique()\n",
        "    c_point3 = val3 not in df[col].unique()\n",
        "    c_point4 = new_col in df.columns\n",
        "\n",
        "    if (\n",
        "        c_point0 and \n",
        "        c_point1 and \n",
        "        c_point2 and \n",
        "        c_point3 and \n",
        "        c_point4\n",
        "    ):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(train, 'education', 'LowEducation', 'SomeHigherEd', 'Prof-school', 'age-hours')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "- `replace()` 함수를 사용하면 값을 재구분할 수 있습니다.\n",
        "- inplace = True 로 설정한다면 원래의 데이터프레임에 적용이 됩니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "```python\n",
        "import pandas as pd  \n",
        "\n",
        "train = pd.read_csv('train.csv')  \n",
        "test = pd.read_csv('test.csv')  \n",
        "\n",
        "# 교육(education) 수준 카테고리 재분류\n",
        "train['education'].replace(['Preschool', '10th', '11th', '12th', '1st-4th', '5th-6th', '7th-8th', '9th'], 'LowEducation', inplace=True)\n",
        "train['education'].replace(['Some-college', 'Assoc-acdm', 'Assoc-voc'], 'SomeHigherEd', inplace=True)\n",
        "train['education'].replace(['Masters', 'Prof-school'], ['Masters', 'Masters'], inplace=True)\n",
        "\n",
        "#결혼 상태 (marital.status) 수준 카테고리 재분류\n",
        "train['marital.status'].replace(['Never.married', 'Married.spouse.absent'], 'UnmarriedStatus', inplace=True)\n",
        "train['marital.status'].replace(['Married.AF.spouse', 'Married.civ.spouse'], 'Married', inplace=True)\n",
        "train['marital.status'].replace(['Separated', 'Divorced'], 'MarriageEnded', inplace=True)\n",
        "\n",
        "# 나이(age)와 주당 근무시간(hours.per.week)을 곱한 새로운 특성 생성\n",
        "train['age-hours'] = train['age']*train['hours.per.week']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.test 데이터에 대한 전처리 및 새로운 특성 생성\n",
        "\n",
        "train 데이터셋에 적용한 데이터 전처리를 test 데이터셋에도 동일하게 적용하여 일관성을 유지하고 모델의 일반화 성능을 향상시킵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# 교육(education) 수준 카테고리 재분류\n",
        "test['education'].replace(['Preschool', '10th', '11th', '12th', '1st-4th', '5th-6th', '7th-8th', '9th'], 'LowEducation', inplace=True)\n",
        "test['education'].replace(['Some-college', 'Assoc-acdm', 'Assoc-voc'], 'SomeHigherEd', inplace=True)\n",
        "test['education'].replace(['Masters', 'Prof-school'], ['Masters', 'Masters'], inplace=True)\n",
        "\n",
        "#결혼 상태 (marital.status) 수준 카테고리 재분류\n",
        "test['marital.status'].replace(['Never.married', 'Married.spouse.absent'], 'UnmarriedStatus', inplace=True)\n",
        "test['marital.status'].replace(['Married.AF.spouse', 'Married.civ.spouse'], 'Married', inplace=True)\n",
        "test['marital.status'].replace(['Separated', 'Divorced'], 'MarriageEnded', inplace=True)\n",
        "\n",
        "# 나이(age)와 주당 근무시간(hours.per.week)을 곱한 새로운 특성 생성\n",
        "test['age-hours'] = test['age']*test['hours.per.week']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#checkcode\n",
        "#empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3.결측치 처리와 불필요한 열 제거\n",
        "\n",
        "[문제 2]\n",
        "- 결측치 처리를 위한 `SimpleImputer` 클래스를 가져오세요.    \n",
        "- `SimpleImputer` 객체를 생성하고, 결측치를 처리할 때 해당 특성에서 가장 자주 발생하는 값으로 결측치를 대체해주세요.\n",
        "- 학습 데이터셋인 `train`의 'occupation'과 'workclass' 열에 대해 결측치를 `최빈값`으로 채워주세요.\n",
        "- 테스트 데이터셋인 `test`의 'occupation'과 'workclass' 열에 대해 결측치를 `최빈값`으로 채워주세요.\n",
        "- train, test 데이터셋에서 불필요한 피처인 `'native.country','ID'` 을 제거해 주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.impute import ___\n",
        "\n",
        "# SimpleImputer를 사용하여 결측치를 최빈값으로 보간\n",
        "imputer = ___(strategy='___')\n",
        "train[['occupation','workclass']] = ___.___(...)\n",
        "test[['occupation','workclass']] = ___.___(...)\n",
        "\n",
        "# 불필요한 열 제거\n",
        "train = train.___(...)\n",
        "test = test.___(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "ensure_vals(globals(), 'x_train', 'imputer')\n",
        "@check_safety\n",
        "def check(\n",
        "    df: pd.DataFrame,\n",
        "    encoder: SimpleImputer,\n",
        "    not_col: str,\n",
        "    use_col1: str,\n",
        "    use_col2: str\n",
        "):\n",
        "    c_point0 = not_col not in df.columns\n",
        "    c_point1 = use_col1 in encoder.feature_names_in_\n",
        "    c_point1 = use_col2 in encoder.feature_names_in_\n",
        "\n",
        "    if c_point0 and c_point1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(train, imputer, 'native.country', 'workclass', 'occupation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "- 전략(strategy) 인자를 'most_frequent'로 설정하면 최빈값으로 결측치를 대체할 수 있습니다.\n",
        "- 학습 데이터셋인 `train`의 결측치를 `최빈값`으로 채우기 위해 `imputer` 객체의 `fit_transform` 메서드를 사용합니다.   \n",
        "- 검증 데이터셋인 `test`의 결측치를 `최빈값`으로 채우기 위해 `imputer` 객체의 `transform` 메서드를 사용합니다.   \n",
        "- `drop()` 함수를 사용하면 특정 칼럼을 제거할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "```python\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# SimpleImputer를 사용하여 결측치를 최빈값으로 보간\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "train[['occupation','workclass']] = imputer.fit_transform(train[['occupation','workclass']])\n",
        "test[['occupation','workclass']] = imputer.transform(test[['occupation','workclass']])\n",
        "\n",
        "# 불필요한 열 제거\n",
        "train = train.drop(['ID','native.country'], axis = 1)\n",
        "test = test.drop(['ID','native.country'], axis = 1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.원-핫 인코딩을 사용한 범주형 변수 변환\n",
        "\n",
        "[문제 3]\n",
        "- scikit-learn의 preprocessing 모듈에서 `OneHotEncoder` 클래스를 불러오세요.\n",
        "- `OneHotEncoder` 객체를 생성합니다. \n",
        "- 학습 데이터셋(`train`)의 `'race', 'sex', 'marital.status'` 열을 선택하고, 원-핫 인코딩을 적용해주세요.\n",
        "- 테스트 데이터셋(`test`)의 `'race', 'sex', 'marital.status'` 열을 선택하고, 원-핫 인코딩을 적용해주세요.\n",
        "- 원-핫 인코딩된 훈련 데이터를 새로운 데이터프레임으로 변환해주세요.   \n",
        "컬럼 이름은 'race_xxx', 'sex_xxx', 'marital.status_xxx' 형태의 이름으로 생성해주세요. 여기서 xxx는 해당 카테고리 변수의 각 범주를 나타냅니다.\n",
        "- test 데이터셋 기존의 인덱스를 삭제하고 새로운 연속적인 정수 인덱스로 설정해주세요.\n",
        "- test와 test_ohe를 열 방향(axis=1)으로 합쳐주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import ___\n",
        "\n",
        "encoder = ___(sparse=False)\n",
        "train_encoded = ___.fit_transform(train[['race', 'sex', 'marital.status']])\n",
        "test_encoded = ___.___(...)\n",
        "\n",
        "# 원-핫 인코딩된 데이터를 DataFrame으로 변환\n",
        "train_ohe = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out(['race', 'sex', 'marital.status']))\n",
        "test_ohe = pd.DataFrame(___, columns=___.___(...))\n",
        "\n",
        "# 데이터프레임에 원-핫 인코딩된 특성 추가\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.___(___=___)\n",
        "\n",
        "train = pd.concat([train,train_ohe], axis=1)\n",
        "test = pd.concat(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "ensure_vals(globals(), 'train', 'test')\n",
        "@check_safety\n",
        "def check(\n",
        "    train_df: pd.DataFrame,\n",
        "    test_df: pd.DataFrame,\n",
        "    enc: OneHotEncoder,\n",
        "    enc_name: str,\n",
        "    num_enc_feature: int,\n",
        "    len_train_col: int,\n",
        "    len_test_col: int\n",
        "):\n",
        "    \n",
        "    c_point0 = enc_name in str(enc)\n",
        "    c_point1 = len(enc.feature_names_in_) == num_enc_feature\n",
        "    c_point2 = len(train_df.columns) == len_train_col\n",
        "    c_point3 = len(test_df.columns) == len_test_col\n",
        "    \n",
        "\n",
        "    if (\n",
        "        c_point0 and \n",
        "        c_point1 and \n",
        "        c_point2 and \n",
        "        c_point3\n",
        "    ):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(train, test, encoder, 'OneHotEncoder', 3, 28, 27)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "- train 데이터셋에는 fit_transform을, test 데이터셋에는 transform을 적용해야 합니다.\n",
        "- get_feature_names_out() 함수를 사용하면 'race_xxx', 'sex_xxx', 'marital.status_xxx' 형태의 이름으로 생성됩니다.\n",
        "- reset_index(drop=True) 함수를 사용하면 기존의 인덱스를 삭제할 수 있습니다.\n",
        "- axis = 1로 설정하면 열 방향으로 합쳐집니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution\n",
        "```python\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "train_encoded = encoder.fit_transform(train[['race', 'sex', 'marital.status']])\n",
        "test_encoded = encoder.transform(test[['race', 'sex', 'marital.status']])\n",
        "\n",
        "# 원-핫 인코딩된 데이터를 DataFrame으로 변환\n",
        "train_ohe = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out(['race', 'sex', 'marital.status']))\n",
        "test_ohe = pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out(['race', 'sex', 'marital.status']))\n",
        "\n",
        "# 데이터프레임에 원-핫 인코딩된 특성 추가\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "\n",
        "train = pd.concat([train,train_ohe], axis=1)\n",
        "test = pd.concat([test,test_ohe], axis=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5.라벨 인코딩을 사용한 범주형 변수 변환\n",
        "\n",
        "[문제 4]   \n",
        "- `LabelEncoder` 객체를 생성하여 le 변수에 할당해 주세요.\n",
        "- x_train 의 현재 열의 타입이 object 인 경우 아래 셀을 실행해 주세요.\n",
        "- `x_train` 데이터프레임의 현재 열에 LabelEncoder을 적용해 주세요.\n",
        "- 만약 검증 데이터에서 새롭게 나타나는 범주값(label)이라면, 이를 인코더의 클래스 목록(le.classes_)에 추가해 주세요.\n",
        "- `x_valid` 데이터프레임의 현재 열에 LabelEncoder을 적용해 주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "for col in train.columns:\n",
        "    if train[col].___ == '___':\n",
        "        \n",
        "        le = ___()\n",
        "        train[col] = ___.___(___[___])\n",
        "\n",
        "        for label in np.unique(test[col]):\n",
        "            if label not in le.classes_:\n",
        "                le.classes_ = np.___(le.classes_, ___)\n",
        "        test[col] = ___.___(___[___])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "ensure_vals(globals(), 'train', 'test')\n",
        "@check_safety\n",
        "def check(\n",
        "    train_df: pd.DataFrame,\n",
        "    test_df: pd.DataFrame,\n",
        "    obj: str,\n",
        "    zero: 0,\n",
        "    enc_name: str\n",
        "):\n",
        "    \n",
        "    len_obj_train = len(train_df.select_dtypes(include=obj).columns)\n",
        "    len_obj_test = len(test_df.select_dtypes(include=obj).columns)\n",
        "    \n",
        "    c_point0 = len_obj_train == zero\n",
        "    c_point1 = len_obj_test == zero\n",
        "    c_point2 = enc_name in str(le)\n",
        "\n",
        "    if c_point0:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(train, test, 'object', 0, 'LabelEncoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "- `sklearn.preprocessing` 모듈에서 `LabelEncoder`라는 클래스를 불러올 수 있습니다.\n",
        "- 열의 타입은 dtype 함수를 이용하면 알 수 있습니다.\n",
        "- `fit_transform` 메서드를 사용하면 LabelEncoder를 적용할 수 있습니다.\n",
        "- `append()` 메서드를 사용하면 클래스 목록에 추가할 수 있습니다.\n",
        "- `transform` 메서드를 사용하여 x_valid 데이터에 Label Encoding을 적용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "for col in train.columns:\n",
        "    if train[col].dtype == 'object':\n",
        "        \n",
        "        le = LabelEncoder()\n",
        "        train[col] = le.fit_transform(train[col])\n",
        "\n",
        "        for label in np.unique(test[col]):\n",
        "            if label not in le.classes_:\n",
        "                le.classes_ = np.append(le.classes_, label)\n",
        "        test[col] = le.transform(test[col])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6.StandardScaler를 이용한 데이터 표준화\n",
        "\n",
        "[문제 5]\n",
        "\n",
        "- StandardScaler() 객체를 생성해 주세요.\n",
        "- train 데이터셋에 해당 피처 부분에 표준화를 적용해주세요.\n",
        "- test 데이터셋에 해당 피처 부분에 표준화를 적용해주세요.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "features = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss','hours.per.week', 'age-hours']\n",
        "\n",
        "scaler = ___()\n",
        "train[features] = ...\n",
        "test[features] = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "ensure_vals(globals(), 'train', 'test', 'scaler')\n",
        "@check_safety\n",
        "def check(\n",
        "    train_df: pd.DataFrame,\n",
        "    test_df: pd.DataFrame,\n",
        "    scaler_class: StandardScaler,\n",
        "    num_features: int,\n",
        "    col1: str,\n",
        "    col2: str\n",
        "):\n",
        "    \n",
        "    c_point0 = len(scaler_class.feature_names_in_) == num_features\n",
        "    c_point1 = train_df[col1].dtypes != int\n",
        "    c_point2 = test_df[col2].dtypes != int\n",
        "\n",
        "    if c_point0 and c_point1 and c_point2:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(train, test, scaler, 7, 'age', 'education.num')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "- train 데이터셋에는 fit_transform을, test 데이터셋에는 transform을 적용해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "features = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss','hours.per.week', 'age-hours']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train[features] = scaler.fit_transform(train[features])\n",
        "test[features] = scaler.transform(test[features])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7.독립 변수와 종속 변수 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_x = train.drop(['target'],axis = 1)\n",
        "train_y = train['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#checkcode\n",
        "#empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8.데이터 차원 축소를 위한 PCA 모델 학습\n",
        "\n",
        "[문제 6]  \n",
        "- train_x 에 대해 PCA 모델을 학습시켜보세요. \n",
        "- `explained_variance_ratio_` 값을 누적해서 더해주세요.\n",
        "- 해당 임계값을 넘는 최초의 인덱스를 추출해 보세요.\n",
        "- 주성분 분석(PCA) 모델을 초기화해주세요. 주성분 개수를 전달해 주세요.\n",
        "- 초기화된 PCA 모델을 기존의 train_x 데이터에 다시 적용해주세요.테스트 데이터인 `test`에 동일한 PCA 모델을 적용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# PCA 모델 학습\n",
        "pca = PCA()\n",
        "pca.___(___)\n",
        "\n",
        "# 분산의 설명량 확인\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# 누적 설명량과 주성분 개수 선택\n",
        "cumulative_explained_variance = ___.___(explained_variance_ratio)\n",
        "n_components = ___.___(___ >= 0.95) + 1\n",
        "\n",
        "# PCA 모델 재설정 및 데이터 변환\n",
        "pca = ___(n_components=___)\n",
        "\n",
        "train_x = pca.___(train_x)\n",
        "test = pca.___(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "import numpy as np\n",
        "ensure_vals(globals(), 'train_x', 'test','pca')\n",
        "@check_safety\n",
        "def check(\n",
        "    train_array: np.array,\n",
        "    test_array: np.array,\n",
        "    pca_class: PCA,\n",
        "    indexing: int\n",
        "):\n",
        "    \n",
        "    c_point0 = len(pca_class.get_feature_names_out()) == train_array.shape[1]\n",
        "    c_point1 = len(pca_class.get_feature_names_out()) == test_array.shape[1]\n",
        "    \n",
        "    if c_point0 and c_point1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check(train_x, test, pca, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n",
        "- fit 함수를 사용하면 학습시킬 수 있습니다.\n",
        "- np.cumsum() 함수를 이용하면 누적합을 구할 수 있습니다.\n",
        "- np.argmax() 함수를 이용하면 해당 임계값을 넘는 최초의 인덱스 값을 구할 수 있습니다.\n",
        "- train_x 데이터셋에는 fit_transform을, test 데이터셋에는 transform을 적용해야 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# PCA 모델 학습\n",
        "pca = PCA()\n",
        "pca.fit(train_x)\n",
        "\n",
        "# 분산의 설명량 확인\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# 누적 설명량과 주성분 개수 선택\n",
        "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "n_components = np.argmax(cumulative_explained_variance >= 0.95) + 1\n",
        "\n",
        "# PCA 모델 재설정 및 데이터 변환\n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "train_x = pca.fit_transform(train_x)\n",
        "test = pca.transform(test)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9.GradientBoostingClassifier 모델을 활용한 데이터 분석 및 성능 평가\n",
        "\n",
        "\n",
        "[문제 8]\n",
        "\n",
        "- GradientBoostingClassifier 모델을 생성해 주세요. \n",
        "- 학습 데이터를 사용하여 학습시켜주세요.\n",
        "- 학습된 모델을 사용하여 테스트 데이터에 대한 예측을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "gb_classifier = ___(random_state=30, \n",
        "                    max_depth=6,\n",
        "                    min_samples_split=3,\n",
        "                    max_features=10)\n",
        "\n",
        "gb_classifier.___(___, ___)\n",
        "\n",
        "y_pred = gb_classifier.___(___)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#checkcode\n",
        "import numpy as np\n",
        "ensure_vals(globals(), 'gb_classifier', '')\n",
        "@check_safety\n",
        "def check(\n",
        "    model_name: str,\n",
        "    model: GradientBoostingClassifier,\n",
        "    train_array: np.ndarray,\n",
        "    one: int,\n",
        "    pred_value: np.ndarray,\n",
        "    pred_shape: tuple\n",
        "):\n",
        "    \n",
        "    c_point0 = model_name in str(model)\n",
        "    c_point1 = model.n_features_in_ == (train_array.shape[one])\n",
        "    c_point2 = pred_value.shape == pred_shape\n",
        "    \n",
        "    if c_point0 and c_point1 and c_point2:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "check('GradientBoostingClassifier', gb_classifier, train_x, 1, y_pred,(494,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.  \n",
        "\n",
        "- random_state: 모델의 랜덤 시드를 설정합니다. 이 값은 모델 학습 시에 무작위성을 제어하기 위해 사용됩니다. 동일한 시드를 사용하면 학습 결과가 재현 가능하게 됩니다.\n",
        "- max_depth: 트리의 최대 깊이를 제한하는 매개변수입니다. 트리의 깊이가 깊어질수록 모델의 복잡성이 증가하므로, 과적합을 방지하기 위해 설정할 수 있습니다.\n",
        "- min_samples_split: 노드를 분할하기 위한 최소한의 샘플 수를 지정하는 매개변수입니다. 이 값보다 작은 수의 샘플을 가진 노드는 더 이상 분할되지 않습니다.\n",
        "- max_features: 각 노드에서 분할에 사용될 최대 특성의 수를 제한하는 매개변수입니다. 이를 통해 트리의 다양성을 제어할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "gb_classifier = GradientBoostingClassifier(random_state=30, \n",
        "                                            max_depth=6,\n",
        "                                            min_samples_split=3,\n",
        "                                            max_features=10)\n",
        "gb_classifier.fit(train_x, train_y)\n",
        "\n",
        "y_pred = gb_classifier.predict(test)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10.데이터프레임(DataFrame)을 CSV 파일로 저장하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {},
      "outputs": [],
      "source": [
        "submission = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "submission['target'] = y_pred\n",
        "submission.to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#checkcode\n",
        "#empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inst.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Solution.\n",
        "empty"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
