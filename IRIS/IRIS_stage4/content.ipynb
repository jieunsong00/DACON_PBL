{"cells":[{"cell_type":"code","execution_count":null,"id":"3f9a618a","metadata":{"id":"3f9a618a"},"outputs":[],"source":["#hiddencell\n","\n","from pbl_tools import *"]},{"cell_type":"markdown","id":"eeffd6d0","metadata":{"id":"eeffd6d0"},"source":["# 스테이지 4. 모델링(1)\n","- Decision Tree 모델 생성\n","- Decision Tree 모델 시각화"]},{"cell_type":"markdown","id":"dae8e2ad","metadata":{},"source":["# 1. csv 파일 불러오기\n","[문제 1]  \n","1. Pandas 라이브러리(library)를 불러와보세요.  \n","2. train.csv, test.csv, sample_submission.csv 파일을 각각 train, test, submission 변수로 읽어오세요.  \n","3. 피처 이름을 변경해 보세요."]},{"cell_type":"code","execution_count":null,"id":"53b3bcfb","metadata":{"id":"53b3bcfb"},"outputs":[],"source":["___ ___ as pd\n","\n","train = pd.___('train.csv')\n","test = pd.___('test.csv')\n","submission = pd.___('sample_submission.csv')\n","\n","feature_names = {\n","    'sepal length (cm)': 'sepal_length',\n","    'petal length (cm)': 'petal_length',\n","    'sepal width (cm)': 'sepal_width',\n","    'petal width (cm)': 'petal_width'\n","}\n","train = train.rename(columns=___)\n","test = test.rename(columns=___)\n","\n","print('train 데이터 개수: ', len(train))\n","print('test 데이터 개수: ', len(test))\n","print('submission 데이터 개수: ', len(submission))"]},{"cell_type":"code","execution_count":null,"id":"f287bd8a","metadata":{"id":"f287bd8a"},"outputs":[],"source":["#checkcode\n","ensure_vals(globals(), 'train', 'test', 'submission')\n","@check_safety\n","def check(\n","    train=train,\n","    test=test,\n","    submission=submission,\n","):\n","    c_point1 = hasattr(train, 'sepal_length')\n","    c_point2 = hasattr(test, 'sepal_width')\n","    c_point3 = hasattr(submission, 'head')\n","\n","    if c_point1 and c_point2 and c_point3:\n","        return True\n","    else:\n","        return False\n","\n","check()"]},{"cell_type":"markdown","id":"3e4c98e9","metadata":{},"source":["### Inst."]},{"cell_type":"markdown","id":"d6da280c","metadata":{},"source":["### Hint.\n","판다스(Pandas)는 `read_csv('파일명')`함수를 이용해 CSV 파일을 불러옵니다.   \n","피처(feature) 이름을 변경할 때에는 `rename` 함수 안에 `columns`을 넣어 줍니다."]},{"cell_type":"markdown","id":"54a3f20e","metadata":{},"source":["### Solution.\n","\n","```python\n","import pandas as pd  \n","\n","train = pd.read_csv('train.csv')  \n","test = pd.read_csv('test.csv')  \n","submission = pd.read_csv('sample_submission.csv')  \n","\n","feature_names = {  \n","    'sepal length (cm)': 'sepal_length',  \n","    'petal length (cm)': 'petal_length',  \n","    'sepal width (cm)': 'sepal_width',  \n","    'petal width (cm)': 'petal_width'  \n","}  \n","train = train.rename(columns=feature_names)  \n","test = test.rename(columns=feature_names)  \n","\n","print('train 데이터 개수: ', len(train))  \n","print('test 데이터 개수: ', len(test))  \n","print('submission 데이터 개수: ', len(submission)) \n","```"]},{"cell_type":"markdown","id":"5006f5c4","metadata":{"id":"5006f5c4"},"source":["# 2. Decision Tree Classifier 모듈 임포트"]},{"cell_type":"code","execution_count":null,"id":"59d5f6ec","metadata":{"id":"59d5f6ec"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier"]},{"cell_type":"code","execution_count":null,"id":"56aa978b","metadata":{"id":"56aa978b"},"outputs":[],"source":["#checkcode\n","#empty"]},{"cell_type":"markdown","id":"07252606","metadata":{"id":"07252606"},"source":["### Inst.\n","\n","의사결정나무(Decision Tree)는 사이킷런에서 제공하는 머신러닝 라이브러리입니다.   \n","이 모델은 지도 학습(Supervised Learning)에서 사용되며, 분류(Classification)와 회귀(Regression) 문제를 해결하는 데 활용할 수 있습니다.\n","\n","의사결정나무는 학습 데이터의 피처(Feature)를 기반으로 분류 기준을 만듭니다.   \n","이를 통해 데이터를 여러 개의 서로 다른 그룹으로 분할합니다.   \n","각 분할은 다시 다른 피처 기준으로 반복적으로 분류를 수행하며, 계속해서 분할을 진행합니다.\n","\n","마지막으로, 분류된 그룹에 대해 예측값을 할당하여 새로운 데이터의 분류를 수행합니다.   \n","이를 통해 의사결정나무는 피처들의 조합에 따라 데이터를 분류하고 예측하는 강력한 모델 중 하나입니다.\n","\n","의사결정나무를 그림으로 그리면 아래와 같습니다.\n","\n","![스크린샷 2023-02-22 오후 5 58 59](https://user-images.githubusercontent.com/39521155/220571524-a0636aaa-419a-45ef-a4d1-bd5bee5414a8.png)"]},{"cell_type":"markdown","id":"6d9ed8d9","metadata":{"id":"6d9ed8d9"},"source":["### Hint.\n","empty"]},{"cell_type":"markdown","id":"a89c1b76","metadata":{"id":"a89c1b76"},"source":["### Solution.\n","empty"]},{"cell_type":"markdown","id":"a70398d6","metadata":{"id":"a70398d6"},"source":["# 3. Decision Tree 모델 생성\n","[문제 2]  \n","`DecisionTreeClassifier`를 사용하여 의사결정나무 분류기 모델을 생성해 보세요."]},{"cell_type":"code","execution_count":null,"id":"10938a14","metadata":{"id":"10938a14"},"outputs":[],"source":["model = ___(max_depth=2, random_state=32)"]},{"cell_type":"code","execution_count":null,"id":"48fc6b82","metadata":{"id":"48fc6b82"},"outputs":[],"source":["#checkcode\n","ensure_vals(globals(),'model')\n","@check_safety\n","def check(\n","    user_answer = model, \n","    user_model = \"<class 'sklearn.tree._classes.DecisionTreeClassifier'>\"\n","    ):\n","\n","    c_point0 = (str(type(user_answer)) == user_model)\n","\n","    if c_point0:\n","        return True\n","    else:\n","        return False\n","check()"]},{"cell_type":"markdown","id":"2cbf1914","metadata":{"id":"2cbf1914"},"source":["### Inst.\n","DecisionTreeClassifier: 사이킷런(scikit-learn)의 의사결정나무 분류기 클래스입니다.   \n","이 클래스는 의사결정나무 모델을 생성하고 학습시킬 수 있습니다.\n","\n","DecisionTreeClassifier의 생성자에 전달하는 인자들을 살펴보겠습니다\n","\n","1. `max_depth=2`: 의사결정나무의 최대 깊이를 2로 설정합니다.       \n","의사결정나무는 분할을 진행하는 깊이를 제한할 수 있는데,   \n","이를 통해 모델이 과적합(overfitting)되는 것을 방지하고 일반화 성능을 향상시킬 수 있습니다.\n","\n","2. `random_state=32`: 의사결정나무 모델의 무작위성을 조정하는 시드(seed) 값입니다.   \n","시드 값을 설정하면 매번 모델이 학습될 때마다 같은 결과를 얻을 수 있습니다."]},{"cell_type":"markdown","id":"76176042","metadata":{"id":"76176042"},"source":["### Hint.\n","empty"]},{"cell_type":"markdown","id":"85309f18","metadata":{"id":"85309f18"},"source":["### Solution.\n","```python\n","model = DecisionTreeClassifier(max_depth=2, random_state=32)\n","```"]},{"cell_type":"markdown","id":"ab8d7e23","metadata":{"id":"ab8d7e23"},"source":["# 4. 학습 데이터 설정\n","\n","[문제 3]\n","\n","1. feature_names: 리스트(list) 형태로 `petal_width`와 `petal_length`라는 피처(feature)의 이름을 저장해 보세요.  \n","2. train_x: train 데이터프레임에서 `feature_names` 피처들을 선택하여 저장해 보세요.\n","3. train_y: train 데이터프레임의 `species` 열을 선택하여 저장해 보세요."]},{"cell_type":"code","execution_count":null,"id":"5e2d9575","metadata":{"id":"5e2d9575"},"outputs":[],"source":["feature_names = ['___', '___']\n","train_x = train[___]\n","train_y = train['___']"]},{"cell_type":"code","execution_count":null,"id":"c10c305d","metadata":{"id":"c10c305d"},"outputs":[],"source":["#checkcode\n","ensure_vals(globals(),'train_x','train_y')\n","@check_safety\n","def check(\n","    user_answer_x = train_x, \n","    user_answer_y = train_y,\n","    user_x_shape = (120,2),\n","    user_y_shape = (120,)\n","    ):\n","\n","    c_point0 = user_answer_x.shape == user_x_shape\n","    c_point1 = user_answer_y.shape == user_y_shape\n","\n","    if c_point0 and c_point1:\n","        return True\n","    else:\n","        return False\n","check()"]},{"cell_type":"markdown","id":"cd5fe901","metadata":{"id":"cd5fe901"},"source":["### Inst.\n","\n","데이터프레임(DataFrame)에 피처(feature) 이름으로 구성된 리스트(list)를 넣어 주면, 해당되는 피처로만 구성할 수 있습니다.   \n","이를 이용해 독립변수(X), 종속변수(Y)를 정의해 봅시다.   \n","\n","스테이지3에서는 'petal_width'와 'petal_length' 두 개의 피처를 사용하여 분류기를 구성하였습니다.   \n","이번 Stage에서 학습 데이터를 설정할 때에도 'petal_width'와 'petal_length' 피처를 선택한 이유는   \n","이 두 피처가 분류 작업에 가장 큰 영향을 미칠 것으로 판단되었기 때문입니다.\n","\n","일반적으로 의사결정나무 모델에서는 피처의 중요도를 고려하여 모델링을 수행합니다.     \n","피처의 중요도는 해당 피처가 분류 결정에 얼마나 큰 영향을 미치는지를 나타내는 지표입니다.     \n","따라서 선택한 피처들이 분류 결정에 가장 큰 영향을 미치는 것으로 판단되어 이를 활용하여 모델을 구축하였습니다.    \n","\n","'petal_width'와 'petal_length' 피처들만을 사용한 이유는 이 두 피처가 붓꽃의 종을 가장 잘 구분할 수 있는   \n","중요한 정보를 제공한다고 판단했기 때문입니다.     \n","더 많은 피처를 사용할 수도 있지만, 때로는 불필요한 피처를 제외하여 모델을 단순화할 수 있습니다.  "]},{"cell_type":"markdown","id":"394ac742","metadata":{"id":"394ac742"},"source":["### Hint.\n","train['피처명']"]},{"cell_type":"markdown","id":"d6cc2e54","metadata":{"id":"d6cc2e54"},"source":["### Solution.\n","```python\n","feature_names = ['petal_width', 'petal_length']\n","train_x = train[feature_names]\n","train_y = train['species']\n","```"]},{"cell_type":"markdown","id":"c9b63905","metadata":{"id":"c9b63905"},"source":["# 5. 모델 학습\n","\n","[문제 4]  \n","독립 변수 `train_x`와 종속 변수 `train_y`를 학습(`fit`)해 보세요.    "]},{"cell_type":"code","execution_count":null,"id":"cccf9ce3","metadata":{"id":"cccf9ce3"},"outputs":[],"source":["model.___(___, ___)"]},{"cell_type":"code","execution_count":null,"id":"689b1edd","metadata":{"id":"689b1edd"},"outputs":[],"source":["#checkcode\n","ensure_vals(globals(),'model')\n","@check_safety\n","def check(\n","    user_answer = _, \n","    user_model = 'DecisionTreeClassifier(max_depth=2, random_state=32)'\n","    ):\n","\n","    c_point0 = (str(user_answer) == user_model)\n","\n","    if c_point0:\n","        return True\n","    else:\n","        return False\n","check()"]},{"cell_type":"markdown","id":"13946b84","metadata":{"id":"13946b84"},"source":["### Inst.\n","`fit(독립변수, 종속변수)` 메서드를 사용해서 모델을 학습시킬 수 있습니다.   \n","  \n","`fit()` 메서드는 모델을 학습시키는 과정에서 독립 변수와 종속 변수 간의 관계를 학습하고,   \n","모델의 내부 매개변수를 조정하여 최적의 예측 모델을 생성합니다.  \n","학습 과정에서 모델은 주어진 입력 데이터에 대한 예측을 수행하고,   \n","실제 출력 데이터와 비교하여 오차를 계산하고 이를 최소화하는 방향으로 내부 매개변수를 조정합니다.   \n","이러한 반복 과정을 통해 모델은 데이터의 패턴과 관계를 학습하여 예측 성능을 향상시킵니다.  "]},{"cell_type":"markdown","id":"7e10b8f2","metadata":{"id":"7e10b8f2"},"source":["### Hint.\n","empty"]},{"cell_type":"markdown","id":"3e50b7ea","metadata":{"id":"3e50b7ea"},"source":["### Solution.\n","```python\n","model.fit(train_x, train_y)\n","```"]},{"cell_type":"markdown","id":"668f1f90","metadata":{"id":"668f1f90"},"source":["# 6. 의사결정나무 모델 시각화\n","\n","tree 모듈을 이용하면 학습한 모델이 어떻게 구성되어 있는지 그림으로 볼 수 있습니다."]},{"cell_type":"code","execution_count":null,"id":"8bcd5852","metadata":{"id":"8bcd5852"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn import tree\n","\n","plt.figure(figsize=(5,5))\n","tree.plot_tree(model, \n","    class_names=['setosa', 'versicolor', 'virginica'],\n","    feature_names=feature_names,\n","    filled=True,\n",")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"f078bef7","metadata":{"id":"f078bef7"},"outputs":[],"source":["#checkcode\n","#empty"]},{"cell_type":"markdown","id":"be775f29","metadata":{"id":"be775f29"},"source":["### Inst.\n","\n","`plt.figure(figsize=(20,15))`: 새로운 그림(figure)을 생성하고, 그림의 크기를 (20, 15)로 설정합니다.   \n","이는 시각화 결과를 표시하는 도표의 크기를 조정하는 역할을 합니다.\n","\n","sklearn.tree 모듈의 plot_tree 함수를 사용하여 의사결정나무 모델의 시각화를 수행합니다.   \n","이 함수는 의사결정나무 모델 객체인 model을 입력으로 받으며,   \n","분류 클래스의 이름(class_names)과 피처의 이름(feature_names)을 지정하여 시각화 결과를 렌더링합니다.   \n","filled=True는 노드를 클래스별로 색상으로 구분하여 채우는 옵션입니다.\n","\n","위 코드를 실행하면 의사결정나무 모델의 구조와 분류 기준을 시각화한 결과가 출력됩니다.   \n","트리의 각 노드는 분할 조건과 클래스 분포를 나타내며, 잎(리프) 노드는 최종적인 클래스를 나타냅니다.\n","\n","---\n","\n","#### 결과 해석\n","\n","> 1. 루트노드(root node)\n","- petal_width가 0.8보다 작거나 같은 샘플은 개수가 많고, 불순도가 높습니다(gini=0.667).\n","> 2. 왼쪽 첫번째 자식노드(left child node)\n","- 이 노드는 매우 순수합니다(samples=40, value = [40, 0, 0]).\n","- petal_width <= 0.8으로 완전히 분류되었습니다.    \n","- 이는 petal_width <= 0.8인 모든 샘플들이 setosa 클래스임을 확인할 수 있습니다.   \n","> 3. 오른쪽 첫번째 자식노드(right child node)\n","- 이 노드는 높은 불순도를 가진 비교적 큰 샘플 총합(80)을 가지고 있으며, 대부분의 샘플들은 versicolor, virginica 클래스에 속합니다.\n","> 4. 오른쪽 첫번째 자식노드의 왼쪽 자식노드(left child node of the right child node)\n","- 이 노드는 매우 순수한(smaples=37, gini=0.053) 것으로 나타났고, 이 노드의 샘플들은 대부분 versicolor 클래스에 속합니다(value=[0,36,1]).\n","> 5. 오른쪽 첫번째 자식노드의 오른쪽 자식노드(right child node of the right child node)\n","- 이 노드는 비교적 높은 불순도(samples=43, gini=0.206)를 보입니다.\n","- 이 노드에 속한 샘플들은 대부분 virginica 클래스에 속하고 있습니다(value=[0, 5, 38])."]},{"cell_type":"markdown","id":"48454ae6","metadata":{"id":"48454ae6"},"source":["### Hint.\n","empty"]},{"cell_type":"markdown","id":"6c2d7b7b","metadata":{"id":"6c2d7b7b"},"source":["### Solution.\n","empty"]},{"cell_type":"markdown","id":"e211ce46","metadata":{"id":"e211ce46"},"source":["# 7. 다른 데이터도 사용\n","\n","petal_width, petal_length, petal_length, petal_width 를 다양하게 사용해서 모델을 학습해 봅시다.   \n","그리고 `max_depth`를 변경하여 모델이 어떻게 바뀌는지 확인해 봅시다."]},{"cell_type":"code","execution_count":null,"id":"bc951d3b","metadata":{"id":"bc951d3b"},"outputs":[],"source":["model = DecisionTreeClassifier(max_depth=5, random_state=32)\n","\n","feature_names = ['petal_width', 'petal_length', 'petal_length', 'petal_width']\n","train_x = train[feature_names]\n","train_y = train['species']\n","\n","model.fit(train_x, train_y)\n","\n","import matplotlib.pyplot as plt\n","from sklearn import tree\n","\n","plt.figure(figsize=(20,15))\n","tree.plot_tree(model, \n","    class_names=['setosa', 'versicolor', 'virginica'],\n","    feature_names=feature_names,\n","    filled=True,\n",")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"1f8927a3","metadata":{"id":"1f8927a3"},"outputs":[],"source":["#checkcode\n","#empty"]},{"cell_type":"markdown","id":"d64570c3","metadata":{"id":"d64570c3"},"source":["### Inst.\n","\n","`feature_names` 변수에 원하는 피처를 넣어 봅시다. 그리고 `max_depth`를 변경하여 트리의 구조를 변경해 봅시다.\n","\n","---\n","\n","#### 결과 해석\n","\n","step6은 DecisionTreeClassifier 모델을 생성할 때 max_depth=2로 설정하여 의사결정나무의 최대 깊이를 2로 제한한 경우입니다.   \n","따라서 생성된 의사결정나무는 최대 2단계의 분할을 수행합니다. 이는 트리의 깊이가 얕고, 더 간단한 모델을 만들어내는 경향이 있습니다.\n","\n","반면, 해당 코드는 max_depth=5로 설정하여 의사결정나무의 최대 깊이를 5로 제한한 경우입니다.   \n","이렇게 설정하면 의사결정나무는 최대 5단계의 분할을 수행하게 됩니다.   \n","따라서 트리의 깊이가 깊어지며, 보다 복잡한 모델이 만들어질 가능성이 높아집니다.\n","\n","max_depth는 의사결정나무의 깊이를 조절하는 하이퍼파라미터입니다.   \n","깊이가 얕은 모델은 데이터의 일반적인 경향을 파악하는데 유용하며, 깊이가 깊은 모델은 더욱 복잡한 패턴을 학습할 수 있습니다.   \n","하지만 깊이가 깊은 모델은 과적합(overfitting)의 가능성이 높으므로 조심해야 합니다.\n","\n","따라서 max_depth를 적절히 조절하여 모델의 복잡성과 일반화 성능 사이의 균형을 유지하는 것이 중요합니다.   \n","깊이가 얕은 모델은 과소적합(underfitting) 문제를 발생시킬 수 있고,   \n","깊이가 깊은 모델은 과적합 문제를 발생시킬 수 있으므로,   \n","적절한 max_depth 값을 찾는 것이 중요한 모델 튜닝 과정입니다."]},{"cell_type":"markdown","id":"d76ad1f8","metadata":{"id":"d76ad1f8"},"source":["### Hint.\n","empty"]},{"cell_type":"markdown","id":"7747dc41","metadata":{"id":"7747dc41"},"source":["### Solution.\n","empty"]}],"metadata":{"colab":{"provenance":[]},"jupytext":{"formats":"ipynb,py:light"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":5}
